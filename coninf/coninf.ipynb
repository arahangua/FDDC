{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring connectivity using covariance based methods \n",
    "\n",
    "In this study, we inferred connectivity using\n",
    "1. Network deconvolution (ND)\n",
    "2. Graphical LASSO (Glasso)\n",
    "3. Differential covariance (Dcov)\n",
    "4. Dynamic differential covariance (DDC)\n",
    "5. Fractional differential covariance (FDDC)\n",
    "\n",
    "Please refer to the manuscript for appropriate references.\n",
    "\n",
    "(*for pairwise inference using CCG, please check the 'matlab' folder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('./python/')\n",
    "sys.path.append('../util')\n",
    "\n",
    "import coninf\n",
    "import util\n",
    "\n",
    "# Load spike trains (here we use a simulation of 5 sec. purely for demonstration purpose, please consider using longer recordings/simulations) \n",
    "data_path = '../example_data/2000/2000_13.0_24.0_5.0/instances/0.npy'\n",
    "data = np.load(data_path, allow_pickle=True).item()\n",
    "\n",
    "# get spiketrains and groundtruth connections for inference\n",
    "spktrain = util.get_spktrains(data_path)\n",
    "gt_conn_dict = util.get_gt_conn(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': array([679,  48, 618, ..., 200, 340, 784], dtype=int32), 'spktrains': array([   0. ,    0. ,    0. , ..., 3999.9, 3999.9, 3999.9])}\n",
      "number of active neurons in this simulation : 1997\n"
     ]
    }
   ],
   "source": [
    "# print out spktrain variable\n",
    "print(spktrain) \n",
    "print('number of active neurons in this simulation : ' + str(len(np.unique(spktrain['ids'])))) # 3 neurons were dormant for this short simulation\n",
    "# ids : spike template ids (neuron id)\n",
    "# spktrains : spike times in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 1999 1999 1999]\n",
      " [  54   93  122 ... 1515 1529 1595]]\n",
      "number of neurons that had at least one connection in this simulation : 2000\n"
     ]
    }
   ],
   "source": [
    "# print out groundtruth connections\n",
    "print(gt_conn_dict['all'])\n",
    "print('number of neurons that had at least one connection in this simulation : '+ str(len(np.unique(gt_conn_dict['all']))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takim/.local/lib/python3.7/site-packages/elephant/conversion.py:835: UserWarning: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "  'behaviour.'.format(num_rounding_corrections))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decomposition and deconvolution...\n",
      "inference successfully finished\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "params = {}\n",
    "# method to use\n",
    "params['method']='nd'\n",
    "# length of data to use \n",
    "params['t_compute'] = int(4) # in seconds \n",
    "# set spike train data\n",
    "params['spktrains']= spktrain['spktrains']\n",
    "params['ids']=spktrain['ids']\n",
    "params['gt_conn']=gt_conn_dict\n",
    "\n",
    "# ND specific params\n",
    "params['covar_bin'] = int(5) # in miliseconds. \n",
    "params['beta'] = 0.99 # for netowrk deconv.\n",
    "\n",
    "\n",
    "nd_result = coninf.run_inference(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['true_con', 'thres_con', 'score_con', 'compute_time', 'sol_mat', 'method_params'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check of the result (*same for all follwing methods)\n",
    "nd_result.keys()\n",
    "# true_con : adjacency matrix (groundtruth connections)\n",
    "# thres_con : not used (for now same as true_con)\n",
    "# score_con : adjacency matrix (estimated score)\n",
    "# compute_time : cpu wall clock time (in seconds)\n",
    "# sol_mat : raw output of the method (i.e. before taking absolute values)\n",
    "# method_params : returns params that were used for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compute_time</th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>thresholds</th>\n",
       "      <th>prc_precision</th>\n",
       "      <th>prc_recall</th>\n",
       "      <th>prc_thresholds</th>\n",
       "      <th>auc</th>\n",
       "      <th>aps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.576842</td>\n",
       "      <td>[0.0, 2.560198589484189e-07, 1.536119153690513...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.2489851995253857e-...</td>\n",
       "      <td>[2.0, 1.0, 0.9753079445722413, 0.9684357774854...</td>\n",
       "      <td>[0.02008649246414712, 0.019785660742561745, 0....</td>\n",
       "      <td>[1.0, 0.6863673265471805, 0.6863673265471805, ...</td>\n",
       "      <td>[0.0, 0.7639089868516508, 0.7639089868516517, ...</td>\n",
       "      <td>0.519834</td>\n",
       "      <td>0.023083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compute_time                                                fpr  \\\n",
       "0      8.576842  [0.0, 2.560198589484189e-07, 1.536119153690513...   \n",
       "\n",
       "                                                 tpr  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 1.2489851995253857e-...   \n",
       "\n",
       "                                          thresholds  \\\n",
       "0  [2.0, 1.0, 0.9753079445722413, 0.9684357774854...   \n",
       "\n",
       "                                       prc_precision  \\\n",
       "0  [0.02008649246414712, 0.019785660742561745, 0....   \n",
       "\n",
       "                                          prc_recall  \\\n",
       "0  [1.0, 0.6863673265471805, 0.6863673265471805, ...   \n",
       "\n",
       "                                      prc_thresholds       auc       aps  \n",
       "0  [0.0, 0.7639089868516508, 0.7639089868516517, ...  0.519834  0.023083  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating inference performance (*same for all following methods)\n",
    "eval_result = util.eval_performance(nd_result)\n",
    "eval_result "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takim/.local/lib/python3.7/site-packages/elephant/conversion.py:835: UserWarning: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "  'behaviour.'.format(num_rounding_corrections))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of empirical covariance matrix:  (1997, 1997)\n",
      "Length of the sampled time series:  800\n",
      " \n",
      "SINGLE GRAPHICAL LASSO PROBLEM \n",
      "Regularization parameters:\n",
      "{'lambda1': 0.05, 'mu1': None}\n",
      "ADMM terminated after 42 iterations with status: optimal.\n",
      "ADMM terminated after 11 iterations with status: optimal.\n",
      "ADMM terminated after 68 iterations with status: optimal.\n",
      "ADMM terminated after 85 iterations with status: optimal.\n",
      "{'lambda1': 0.03162277660168379, 'mu1': 0}\n",
      "inference successfully finished\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "params = {}\n",
    "# method to use\n",
    "params['method']='glasso'\n",
    "# length of data to use \n",
    "params['t_compute'] = int(4) # in seconds \n",
    "# set spike train data\n",
    "params['spktrains']= spktrain['spktrains']\n",
    "params['ids']=spktrain['ids']\n",
    "params['gt_conn']=gt_conn_dict\n",
    "\n",
    "# glasso specific params\n",
    "params['covar_bin'] = int(5) # for binning\n",
    "params['gamma']=0.1 # regularization parameter\n",
    "\n",
    "\n",
    "glasso_result = coninf.run_inference(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takim/.local/lib/python3.7/site-packages/elephant/conversion.py:835: UserWarning: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "  'behaviour.'.format(num_rounding_corrections))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference successfully finished\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "params = {}\n",
    "# method to use\n",
    "params['method']='dcov'\n",
    "# length of data to use \n",
    "params['t_compute'] = int(4) # in seconds \n",
    "# set spike train data\n",
    "params['spktrains']= spktrain['spktrains']\n",
    "params['ids']=spktrain['ids']\n",
    "params['gt_conn']=gt_conn_dict\n",
    "\n",
    "# Dcov specific params\n",
    "params['covar_bin'] = int(5) # for binning\n",
    "\n",
    "\n",
    "dcov_result = coninf.run_inference(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic differential covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1997, 800)\n",
      "inference successfully finished\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "params = {}\n",
    "# method to use\n",
    "params['method']='ddc'\n",
    "# length of data to use \n",
    "params['t_compute'] = int(4) # in seconds \n",
    "# set spike train data\n",
    "params['spktrains']= spktrain['spktrains']\n",
    "params['ids']=spktrain['ids']\n",
    "params['gt_conn']=gt_conn_dict\n",
    "\n",
    "# Dcov specific params\n",
    "params['covar_bin'] = int(5) # for binning\n",
    "params['kernel']= 'gauss' # 'alpha' or 'gauss'\n",
    "params['kernel_size']=int(3) # in ms. \n",
    "\n",
    "\n",
    "ddc_result = coninf.run_inference(params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractional dynamic differential covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1997, 800)\n",
      "computing fractional diff with dimension of 0.1\n",
      "inference successfully finished\n"
     ]
    }
   ],
   "source": [
    "# setting parameters\n",
    "\n",
    "params = {}\n",
    "# method to use\n",
    "params['method']='frac_ddc'\n",
    "# length of data to use \n",
    "params['t_compute'] = int(4) # in seconds \n",
    "# set spike train data\n",
    "params['spktrains']= spktrain['spktrains']\n",
    "params['ids']=spktrain['ids']\n",
    "params['gt_conn']=gt_conn_dict\n",
    "\n",
    "# FDDC specific params\n",
    "params['covar_bin'] = int(5) # for binning\n",
    "params['kernel']= 'alpha' # 'alpha' or 'gauss'\n",
    "params['kernel_size']=int(3) # in ms. \n",
    "params['frac_order']=float(0.1) # fractional order ('beta' in the manuscript)\n",
    "params['diff_window']=int(10) # how many terms will be used to compute fractional differentiation, k=10 in the manuscript.\n",
    "\n",
    "# if frac_order is not set (perform Adfuller test on sampled neurons): \n",
    "params['sample_n']= int(100) # number of neurons to sample \n",
    "params['sample_window']= int(300) # time window (number of bins) to sample \n",
    "\n",
    "\n",
    "fddc_result = coninf.run_inference(params)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
